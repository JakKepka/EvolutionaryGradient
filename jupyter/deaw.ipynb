{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wine DATASET"
      ],
      "metadata": {
        "id": "kbq0NC-Mp0SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Przygotowanie danych: wczytanie, skalowanie, podział i DataLoadery ---\n",
        "\n",
        "# Wczytanie danych\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standaryzacja cech\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Podział na zbiór train+val (60%) i test (40%) z zachowaniem rozkładu klas (stratyfikacja)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.40, random_state=42\n",
        ")\n",
        "\n",
        "# Podział train+val na train (60%) i val (20%) także stratyfikacja\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, stratify=y_train_val, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Konwersja do tensorów PyTorch\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Tworzenie datasetów i DataLoaderów\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "V35ZbSkcpz9p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEAW"
      ],
      "metadata": {
        "id": "UygnAJUTvfra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xjrbqhCvvMTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a0c238-6ebc-4f38-92f3-d8a4532c7e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing NP=30, F=0.3, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=30, F=0.3, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.849057\n",
            "Testing NP=30, F=0.3, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.981132\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.867925\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.773585\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.735849\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.660377\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.5, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.5, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.5, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.5, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.867925\n",
            "Testing NP=30, F=0.5, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.811321\n",
            "Testing NP=30, F=0.5, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=30, F=0.8, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.830189\n",
            "Testing NP=30, F=0.8, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.867925\n",
            "Testing NP=30, F=0.8, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.8, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.8, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=30, F=0.8, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.8, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.8, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.8, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=50, F=0.3, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=50, F=0.3, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=50, F=0.3, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.3, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.3, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=50, F=0.3, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=50, F=0.3, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.867925\n",
            "Testing NP=50, F=0.3, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=50, F=0.3, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.5, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=50, F=0.5, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.981132\n",
            "Testing NP=50, F=0.5, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=50, F=0.5, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.5, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.981132\n",
            "Testing NP=50, F=0.5, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=50, F=0.5, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.5, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.5, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.8, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=50, F=0.8, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=50, F=0.8, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=50, F=0.8, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.8, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=50, F=0.8, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=50, F=0.8, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=50, F=0.8, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.849057\n",
            "Testing NP=50, F=0.8, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=70, F=0.3, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=70, F=0.3, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.3, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.3, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.3, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=70, F=0.3, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.981132\n",
            "Testing NP=70, F=0.3, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.811321\n",
            "Testing NP=70, F=0.3, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.792453\n",
            "Testing NP=70, F=0.3, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.5, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.830189\n",
            "Testing NP=70, F=0.5, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=70, F=0.5, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=70, F=0.5, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.5, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=70, F=0.5, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=70, F=0.5, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing NP=70, F=0.5, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=70, F=0.5, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=70, F=0.8, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.867925\n",
            "Testing NP=70, F=0.8, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.811321\n",
            "Testing NP=70, F=0.8, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=70, F=0.8, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=70, F=0.8, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=70, F=0.8, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=70, F=0.8, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.849057\n",
            "Testing NP=70, F=0.8, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=70, F=0.8, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "\n",
            "Best hyperparameters: {'NP': 30, 'F': 0.3, 'CR': 0.7, 'max_generations': 150}\n",
            "Best validation accuracy: 0.981132\n",
            "Final training accuracy: 1.000000\n",
            "Final validation accuracy: 0.981132\n",
            "Final test accuracy: 0.916667\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import product\n",
        "\n",
        "# MLP for Wine dataset (13-16-3 architecture)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=13, hidden_size=16, output_size=3):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        # Initialize weights in [-1, 1]\n",
        "        nn.init.uniform_(self.fc1.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc1.bias, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.bias, -1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation on output\n",
        "        return x\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Flatten weights and biases into a vector\n",
        "        return torch.cat([\n",
        "            self.fc1.weight.flatten(),\n",
        "            self.fc1.bias.flatten(),\n",
        "            self.fc2.weight.flatten(),\n",
        "            self.fc2.bias.flatten()\n",
        "        ])\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        # Set weights from a flat vector\n",
        "        idx = 0\n",
        "        w1_size = self.input_size * self.hidden_size\n",
        "        self.fc1.weight.data = weights[idx:idx+w1_size].reshape(self.hidden_size, self.input_size)\n",
        "        idx += w1_size\n",
        "        self.fc1.bias.data = weights[idx:idx+self.hidden_size]\n",
        "        idx += self.hidden_size\n",
        "        w2_size = self.hidden_size * self.output_size\n",
        "        self.fc2.weight.data = weights[idx:idx+w2_size].reshape(self.output_size, self.hidden_size)\n",
        "        idx += w2_size\n",
        "        self.fc2.bias.data = weights[idx:idx+self.output_size]\n",
        "\n",
        "# Compute accuracy for evaluation\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Fitness function using cross-entropy loss\n",
        "def fitness(weights, model, train_loader, device):\n",
        "    model.set_weights(weights.to(device))\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "# DEAW Algorithm\n",
        "def train_deaw(model, train_loader, device, NP=50, F=0.5, CR=0.9, max_generations=100, initial_lower=-1.0, initial_upper=1.0):\n",
        "    num_weights = sum(p.numel() for p in model.parameters())\n",
        "    lower_bounds = np.full(num_weights, initial_lower)\n",
        "    upper_bounds = np.full(num_weights, initial_upper)\n",
        "    population = np.random.uniform(initial_lower, initial_upper, (NP, num_weights))\n",
        "    fitnesses = np.array([fitness(torch.tensor(p, dtype=torch.float32), model, train_loader, device) for p in population])\n",
        "\n",
        "    for generation in range(max_generations):\n",
        "        for i in range(NP):\n",
        "            candidates = [j for j in range(NP) if j != i]\n",
        "            a, b, c = np.random.choice(candidates, 3, replace=False)\n",
        "            v = population[a] + F * (population[b] - population[c])\n",
        "\n",
        "            for j in range(num_weights):\n",
        "                if v[j] < lower_bounds[j]:\n",
        "                    lower_bounds[j] *= 3\n",
        "                    v[j] = lower_bounds[j]\n",
        "                elif v[j] > upper_bounds[j]:\n",
        "                    upper_bounds[j] *= 3\n",
        "                    v[j] = upper_bounds[j]\n",
        "\n",
        "            u = np.copy(population[i])\n",
        "            j_rand = np.random.randint(0, num_weights)\n",
        "            for j in range(num_weights):\n",
        "                if np.random.rand() < CR or j == j_rand:\n",
        "                    u[j] = v[j]\n",
        "\n",
        "            u_tensor = torch.tensor(u, dtype=torch.float32)\n",
        "            loss_u = fitness(u_tensor, model, train_loader, device)\n",
        "\n",
        "            if loss_u < fitnesses[i]:\n",
        "                population[i] = u.copy()\n",
        "                fitnesses[i] = loss_u\n",
        "\n",
        "    best_idx = np.argmin(fitnesses)\n",
        "    best_weights = torch.tensor(population[best_idx], dtype=torch.float32)\n",
        "    model.set_weights(best_weights.to(device))\n",
        "    return model\n",
        "\n",
        "# Grid search for hyperparameter optimization using validation set\n",
        "def grid_search_deaw(train_loader, val_loader, device):\n",
        "    # Define hyperparameter grid\n",
        "    param_grid = {\n",
        "        'NP': [30, 50, 70],\n",
        "        'F': [0.3, 0.5, 0.8],\n",
        "        'CR': [0.7, 0.9, 1.0],\n",
        "        'max_generations': [50, 100, 150]\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Iterate over all combinations of hyperparameters\n",
        "    for NP, F, CR, max_generations in product(param_grid['NP'], param_grid['F'], param_grid['CR'], param_grid['max_generations']):\n",
        "        print(f\"Testing NP={NP}, F={F}, CR={CR}, max_generations={max_generations}\")\n",
        "        model = MLP(input_size=13, hidden_size=16, output_size=3).to(device)\n",
        "        model = train_deaw(model, train_loader, device, NP=NP, F=F, CR=CR, max_generations=max_generations)\n",
        "        val_accuracy = compute_accuracy(model, val_loader, device)\n",
        "        print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_params = {'NP': NP, 'F': F, 'CR': CR, 'max_generations': max_generations}\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"\\nBest hyperparameters: {best_params}\")\n",
        "    print(f\"Best validation accuracy: {best_accuracy:.6f}\")\n",
        "    return best_model, best_params, best_accuracy\n",
        "\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Perform grid search using validation set\n",
        "best_model, best_params, best_val_accuracy = grid_search_deaw(train_loader, val_loader, device)\n",
        "\n",
        "# Evaluate final model on train, validation, and test sets\n",
        "train_accuracy = compute_accuracy(best_model, train_loader, device)\n",
        "val_accuracy = compute_accuracy(best_model, val_loader, device)\n",
        "test_accuracy = compute_accuracy(best_model, test_loader, device)\n",
        "print(f\"Final training accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Final validation accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Final test accuracy: {test_accuracy:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "accuracy = evaluate_model(model, test_loader, device)\n",
        "print(f\"Dokładność na zbiorze testowym: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzLjXlh2x0AY",
        "outputId": "60c26cab-062a-4afc-d347-7f828273cd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność na zbiorze testowym: 95.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_de(model, train_loader, device, NP=50, F=0.5, CR=0.9, max_generations=100, initial_lower=-1.0, initial_upper=1.0):\n",
        "    num_weights = sum(p.numel() for p in model.parameters())\n",
        "    population = np.random.uniform(initial_lower, initial_upper, (NP, num_weights))\n",
        "    fitnesses = np.array([fitness(torch.tensor(p, dtype=torch.float32), model, train_loader, device) for p in population])\n",
        "\n",
        "    for generation in range(max_generations):\n",
        "        for i in range(NP):\n",
        "            candidates = [j for j in range(NP) if j != i]\n",
        "            a, b, c = np.random.choice(candidates, 3, replace=False)\n",
        "            v = population[a] + F * (population[b] - population[c])\n",
        "\n",
        "            # Stałe ograniczenie: ograniczamy wartości do zakresu\n",
        "            v = np.clip(v, initial_lower, initial_upper)\n",
        "\n",
        "            u = np.copy(population[i])\n",
        "            j_rand = np.random.randint(0, num_weights)\n",
        "            for j in range(num_weights):\n",
        "                if np.random.rand() < CR or j == j_rand:\n",
        "                    u[j] = v[j]\n",
        "\n",
        "            u_tensor = torch.tensor(u, dtype=torch.float32)\n",
        "            loss_u = fitness(u_tensor, model, train_loader, device)\n",
        "\n",
        "            if loss_u < fitnesses[i]:\n",
        "                population[i] = u.copy()\n",
        "                fitnesses[i] = loss_u\n",
        "\n",
        "    best_idx = np.argmin(fitnesses)\n",
        "    best_weights = torch.tensor(population[best_idx], dtype=torch.float32)\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        numel = param.numel()\n",
        "        param.data = best_weights[idx:idx+numel].view(param.size()).to(device)\n",
        "        idx += numel\n"
      ],
      "metadata": {
        "id": "KaPDfg5Ux2Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_de = MLP().to(device)\n",
        "train_de(model_de, train_loader, device)\n",
        "accuracy_de = evaluate_model(model_de, test_loader, device)\n",
        "print(f\"[DE] Dokładność na zbiorze testowym: {accuracy_de:.2f}%\")\n"
      ],
      "metadata": {
        "id": "UhavMND098YU",
        "outputId": "7da8ba66-bef2-4ede-f49b-173b6f98513d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DE] Dokładność na zbiorze testowym: 69.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam"
      ],
      "metadata": {
        "id": "WQYC5hFBNXR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import Adam\n",
        "# Ewaluacja\n",
        "def evaluate(model, X_test, y_test, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test.to(device))\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    acc = accuracy_score(y_test.numpy(), preds)\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Uczenie Adamem\n",
        "model_adam = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model_adam.parameters(), lr=0.01)\n",
        "n_epochs = 50\n",
        "\n",
        "print(\"Training with Adam...\")\n",
        "for epoch in range(n_epochs):\n",
        "    model_adam.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_adam(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "evaluate(model_adam, X_test, y_test, device)"
      ],
      "metadata": {
        "id": "BNITxUn2NWun",
        "outputId": "f4efcca4-63d3-44d1-f41d-21b0db61cd80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam...\n",
            "Test Accuracy: 0.9206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDEADAM"
      ],
      "metadata": {
        "id": "Se7E7wsRuD57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.stats import cauchy\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# MLP for Wine dataset (13-16-3 architecture)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=13, hidden_size=16, output_size=3):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        # Initialize weights in [-1, 1]\n",
        "        nn.init.uniform_(self.fc1.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc1.bias, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.bias, -1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation on output for cross-entropy\n",
        "        return x\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Flatten weights and biases into a vector\n",
        "        return torch.cat([\n",
        "            self.fc1.weight.flatten(),\n",
        "            self.fc1.bias.flatten(),\n",
        "            self.fc2.weight.flatten(),\n",
        "            self.fc2.bias.flatten()\n",
        "        ])\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        # Set weights from a flat vector\n",
        "        idx = 0\n",
        "        w1_size = self.input_size * self.hidden_size\n",
        "        self.fc1.weight.data = weights[idx:idx+w1_size].reshape(self.hidden_size, self.input_size)\n",
        "        idx += w1_size\n",
        "        self.fc1.bias.data = weights[idx:idx+self.hidden_size]\n",
        "        idx += self.hidden_size\n",
        "        w2_size = self.hidden_size * self.output_size\n",
        "        self.fc2.weight.data = weights[idx:idx+w2_size].reshape(self.output_size, self.hidden_size)\n",
        "        idx += w2_size\n",
        "        self.fc2.bias.data = weights[idx:idx+self.output_size]\n",
        "\n",
        "# Compute cross-entropy loss (used for optimization)\n",
        "def compute_loss(model, inputs, targets):\n",
        "    outputs = model(inputs)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    return loss_fn(outputs, targets).item()\n",
        "\n",
        "# Compute accuracy (for evaluation, for torch.long labels)\n",
        "def compute_accuracy(model, inputs, targets):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct = (predicted == targets).float().sum()\n",
        "        accuracy = correct / inputs.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "# Population-based Adam (P-Adam)\n",
        "class PAdam:\n",
        "    def __init__(self, population, alpha=0.1, gamma1=0.9, gamma2=0.99, gamma3=0.999, tau=1e-7):\n",
        "        self.population = population\n",
        "        self.alpha = alpha\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.gamma3 = gamma3\n",
        "        self.tau = tau\n",
        "        self.m = [torch.zeros_like(ind) for ind in population]  # First moment\n",
        "        self.n = [torch.zeros_like(ind) for ind in population]  # Second moment\n",
        "\n",
        "    def step(self, model, inputs, targets, t):\n",
        "        new_population = []\n",
        "        fitnesses = []\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for i, (ind, m_i, n_i) in enumerate(zip(self.population, self.m, self.n)):\n",
        "            model.set_weights(ind)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # Compute gradients\n",
        "            grads = torch.cat([\n",
        "                model.fc1.weight.grad.flatten(),\n",
        "                model.fc1.bias.grad.flatten(),\n",
        "                model.fc2.weight.grad.flatten(),\n",
        "                model.fc2.bias.grad.flatten()\n",
        "            ])\n",
        "\n",
        "            # Update moments\n",
        "            m_i = self.gamma1 * m_i + (1 - self.gamma1) * grads\n",
        "            n_i = self.gamma2 * n_i + (1 - self.gamma3) * (grads ** 2)\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m_i / (1 - self.gamma1 ** t)\n",
        "            n_hat = n_i / (1 - self.gamma3 ** t)\n",
        "\n",
        "            # Update parameters\n",
        "            new_ind = ind - self.alpha * m_hat / (torch.sqrt(n_hat) + self.tau)\n",
        "            new_population.append(new_ind)\n",
        "            fitnesses.append(compute_loss(model, inputs, targets))\n",
        "\n",
        "            # Zero gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            self.m[i] = m_i\n",
        "            self.n[i] = n_i\n",
        "\n",
        "        self.population = new_population\n",
        "        return fitnesses\n",
        "\n",
        "# Modified CoBiDE (M-CoBiDE)\n",
        "class MCoBiDE:\n",
        "    def __init__(self, population, pb=0.5, ps=0.4):\n",
        "        self.population = population\n",
        "        self.pb = pb\n",
        "        self.ps = ps\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.F = [self._sample_F() for _ in population]\n",
        "        self.CR = [self._sample_CR() for _ in population]\n",
        "\n",
        "    def _sample_F(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            return cauchy.rvs(loc=0.65, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            return cauchy.rvs(loc=1.0, scale=0.1, random_state=self.rng)\n",
        "\n",
        "    def _sample_CR(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            cr = cauchy.rvs(loc=0.1, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            cr = cauchy.rvs(loc=0.95, scale=0.1, random_state=self.rng)\n",
        "        return np.clip(cr, 0, 1)\n",
        "\n",
        "    def step(self, model, inputs, targets):\n",
        "        fitnesses = [compute_loss(model, inputs, targets) for ind in self.population]\n",
        "        best_idx = np.argmin(fitnesses)\n",
        "        new_population = []\n",
        "\n",
        "        # Compute covariance matrix for top ps proportion\n",
        "        top_indices = np.argsort(fitnesses)[:int(self.ps * len(self.population))]\n",
        "        top_pop = torch.stack([self.population[i] for i in top_indices])\n",
        "        cov = torch.cov(top_pop.T)\n",
        "        cov += 1e-6 * torch.eye(cov.shape[0])  # Add perturbation for stability\n",
        "        eigvals, eigvecs = torch.linalg.eigh(cov)\n",
        "        P = eigvecs\n",
        "\n",
        "        for i, (ind, F_i, CR_i) in enumerate(zip(self.population, self.F, self.CR)):\n",
        "            r1, r2 = self.rng.choice([j for j in range(len(self.population)) if j != i], 2, replace=False)\n",
        "            v_i = ind + F_i * (self.population[best_idx] - ind) + F_i * (self.population[r1] - self.population[r2])\n",
        "\n",
        "            r3 = self.rng.random()\n",
        "            if r3 >= self.pb:\n",
        "                u_i = ind.clone()\n",
        "                j_rand = self.rng.integers(0, len(ind))\n",
        "                for j in range(len(ind)):\n",
        "                    if self.rng.random() <= CR_i or j == j_rand:\n",
        "                        u_i[j] = v_i[j]\n",
        "            else:\n",
        "                x_prime = P.T @ ind\n",
        "                v_prime = P.T @ v_i\n",
        "                u_prime = x_prime.clone()\n",
        "                j_rand = self.rng.integers(0, len(ind))\n",
        "                for j in range(len(ind)):\n",
        "                    if self.rng.random() <= CR_i or j == j_rand:\n",
        "                        u_prime[j] = v_prime[j]\n",
        "                u_i = P @ u_prime\n",
        "\n",
        "            model.set_weights(u_i)\n",
        "            u_fitness = compute_loss(model, inputs, targets)\n",
        "            if u_fitness < fitnesses[i]:\n",
        "                new_population.append(u_i)\n",
        "                self.F[i] = self._sample_F()\n",
        "                self.CR[i] = self._sample_CR()\n",
        "            else:\n",
        "                new_population.append(ind)\n",
        "\n",
        "        self.population = new_population\n",
        "        return fitnesses\n",
        "\n",
        "# EDEAdam Algorithm\n",
        "class EDEAdam:\n",
        "    def __init__(self, model, pop_size=50, max_evals=25000, exchange_interval=5):\n",
        "        self.model = model\n",
        "        self.pop_size = pop_size\n",
        "        self.max_evals = max_evals\n",
        "        self.exchange_interval = exchange_interval\n",
        "        self.dim = sum(p.numel() for p in model.parameters())\n",
        "        # Verify dimension\n",
        "        expected_dim = model.input_size * model.hidden_size + model.hidden_size + model.hidden_size * model.output_size + model.output_size\n",
        "        assert self.dim == expected_dim, f\"Dimension mismatch: got {self.dim}, expected {expected_dim}\"\n",
        "        # Initialize population\n",
        "        self.population = [torch.rand(self.dim) * 2 - 1 for _ in range(pop_size)]\n",
        "        self.sub_pop1 = self.population[:pop_size//2]\n",
        "        self.sub_pop2 = self.population[pop_size//2:]\n",
        "        self.p_adam = PAdam(self.sub_pop1)\n",
        "        self.m_cobide = MCoBiDE(self.sub_pop2)\n",
        "\n",
        "    def run(self, inputs, targets):\n",
        "        t = 1\n",
        "        eval_count = 0\n",
        "        best_fitness = float('inf')\n",
        "        best_individual = None\n",
        "\n",
        "        while eval_count < self.max_evals:\n",
        "            fitness1 = self.p_adam.step(self.model, inputs, targets, t)\n",
        "            fitness2 = self.m_cobide.step(self.model, inputs, targets)\n",
        "            eval_count += len(self.sub_pop1) + len(self.sub_pop2)\n",
        "\n",
        "            best_idx1, worst_idx1 = np.argmin(fitness1), np.argmax(fitness1)\n",
        "            best_idx2, worst_idx2 = np.argmin(fitness2), np.argmax(fitness2)\n",
        "\n",
        "            if min(fitness1 + fitness2) < best_fitness:\n",
        "                best_fitness = min(fitness1 + fitness2)\n",
        "                best_individual = self.sub_pop1[best_idx1] if fitness1[best_idx1] < fitness2[best_idx2] else self.sub_pop2[best_idx2]\n",
        "\n",
        "            if t % self.exchange_interval == 0:\n",
        "                if fitness1[best_idx1] < fitness2[worst_idx2]:\n",
        "                    self.sub_pop2[worst_idx2] = self.sub_pop1[best_idx1].clone()\n",
        "                if fitness2[best_idx2] < fitness1[worst_idx1]:\n",
        "                    self.sub_pop1[worst_idx1] = self.sub_pop2[best_idx2].clone()\n",
        "\n",
        "            t += 1\n",
        "\n",
        "        self.model.set_weights(best_individual)\n",
        "        return best_fitness, eval_count\n",
        "\n",
        "# Load and preprocess Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target  # Use class indices (torch.long)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "pop_size_values = [25, 50, 100, 250]\n",
        "max_evals_values = [500, 1000]\n",
        "exchange_interval_values = [ 5, 10, 25, 50, 100]\n",
        "\n",
        "# Grid search\n",
        "best_val_accuracy = 0\n",
        "best_params = {}\n",
        "best_model_state = None\n",
        "best_fitness = float('inf')\n",
        "best_eval_count = 0\n",
        "\n",
        "for pop_size in pop_size_values:\n",
        "    for max_evals in max_evals_values:\n",
        "        for exchange_interval in exchange_interval_values:\n",
        "            print(f\"Testing: pop_size={pop_size}, max_evals={max_evals}, exchange_interval={exchange_interval}\")\n",
        "            model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "            ede_adam = EDEAdam(model, pop_size=pop_size, max_evals=max_evals, exchange_interval=exchange_interval)\n",
        "            fitness, eval_count = ede_adam.run(X_train, y_train)\n",
        "            val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "            print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_params = {\n",
        "                    'pop_size': pop_size,\n",
        "                    'max_evals': max_evals,\n",
        "                    'exchange_interval': exchange_interval\n",
        "                }\n",
        "                best_model_state = model.state_dict()\n",
        "                best_fitness = fitness\n",
        "                best_eval_count = eval_count\n",
        "\n",
        "# Load best model state\n",
        "model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate on train, validation, and test sets\n",
        "model.eval()\n",
        "train_accuracy = compute_accuracy(model, X_train, y_train)\n",
        "val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "test_accuracy = compute_accuracy(model, X_test, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"pop_size: {best_params['pop_size']}\")\n",
        "print(f\"max_evals: {best_params['max_evals']}\")\n",
        "print(f\"exchange_interval: {best_params['exchange_interval']}\")\n",
        "print(f\"Best training cross-entropy loss: {best_fitness:.6f}\")\n",
        "print(f\"Training accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.6f}\")\n",
        "print(f\"Total evaluations: {best_eval_count}\")"
      ],
      "metadata": {
        "id": "j_s6i_g89_tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581b3249-4172-4d72-c2ca-7727450ae665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: pop_size=25, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.094340\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.716981\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.169811\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.433962\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.584906\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.452830\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.339623\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.150943\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.509434\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.207547\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.509434\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.528302\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.584906\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.358491\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.396226\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.603774\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.301887\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.415094\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.471698\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.377358\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.547170\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.528302\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.547170\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.660377\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.358491\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.415094\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.735849\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.566038\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.547170\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.528302\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.698113\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.584906\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.698113\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.679245\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.490566\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.584906\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.358491\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.471698\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.603774\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.867925\n",
            "\n",
            "Best Hyperparameters:\n",
            "pop_size: 250\n",
            "max_evals: 1000\n",
            "exchange_interval: 100\n",
            "Best training cross-entropy loss: 0.003298\n",
            "Training accuracy: 0.867925\n",
            "Validation accuracy: 0.867925\n",
            "Test accuracy: 0.888889\n",
            "Total evaluations: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.stats import cauchy\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# MLP for Wine dataset (13-16-3 architecture)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=13, hidden_size=16, output_size=3):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        # Initialize weights in [-1, 1]\n",
        "        nn.init.uniform_(self.fc1.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc1.bias, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.weight, -1, 1)\n",
        "        nn.init.uniform_(self.fc2.bias, -1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation on output for MSE\n",
        "        return x\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Flatten weights and biases into a vector\n",
        "        return torch.cat([\n",
        "            self.fc1.weight.flatten(),\n",
        "            self.fc1.bias.flatten(),\n",
        "            self.fc2.weight.flatten(),\n",
        "            self.fc2.bias.flatten()\n",
        "        ])\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        # Set weights from a flat vector\n",
        "        idx = 0\n",
        "        w1_size = self.input_size * self.hidden_size\n",
        "        self.fc1.weight.data = weights[idx:idx+w1_size].reshape(self.hidden_size, self.input_size)\n",
        "        idx += w1_size\n",
        "        self.fc1.bias.data = weights[idx:idx+self.hidden_size]\n",
        "        idx += self.hidden_size\n",
        "        w2_size = self.hidden_size * self.output_size\n",
        "        self.fc2.weight.data = weights[idx:idx+w2_size].reshape(self.output_size, self.hidden_size)\n",
        "        idx += w2_size\n",
        "        self.fc2.bias.data = weights[idx:idx+self.output_size]\n",
        "\n",
        "# Compute MSE loss (used for optimization)\n",
        "def compute_mse(model, inputs, targets):\n",
        "    outputs = model(inputs)\n",
        "    mse = torch.mean((outputs - targets) ** 2)\n",
        "    return mse.item()\n",
        "\n",
        "# Compute accuracy (for evaluation, for torch.long labels)\n",
        "def compute_accuracy(model, inputs, targets):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct = (predicted == targets).float().sum()\n",
        "        accuracy = correct / inputs.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "# Population-based Adam (P-Adam)\n",
        "class PAdam:\n",
        "    def __init__(self, population, alpha=0.1, gamma1=0.9, gamma2=0.99, gamma3=0.999, tau=1e-7):\n",
        "        self.population = population\n",
        "        self.alpha = alpha\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.gamma3 = gamma3\n",
        "        self.tau = tau\n",
        "        self.m = [torch.zeros_like(ind) for ind in population]  # First moment\n",
        "        self.n = [torch.zeros_like(ind) for ind in population]  # Second moment\n",
        "\n",
        "    def step(self, model, inputs, targets, t):\n",
        "        new_population = []\n",
        "        fitnesses = []\n",
        "        for i, (ind, m_i, n_i) in enumerate(zip(self.population, self.m, self.n)):\n",
        "            model.set_weights(ind)\n",
        "            outputs = model(inputs)\n",
        "            loss = torch.mean((outputs - targets) ** 2)\n",
        "            loss.backward()\n",
        "\n",
        "            # Compute gradients\n",
        "            grads = torch.cat([\n",
        "                model.fc1.weight.grad.flatten(),\n",
        "                model.fc1.bias.grad.flatten(),\n",
        "                model.fc2.weight.grad.flatten(),\n",
        "                model.fc2.bias.grad.flatten()\n",
        "            ])\n",
        "\n",
        "            # Update moments\n",
        "            m_i = self.gamma1 * m_i + (1 - self.gamma1) * grads\n",
        "            n_i = self.gamma2 * n_i + (1 - self.gamma3) * (grads ** 2)\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m_i / (1 - self.gamma1 ** t)\n",
        "            n_hat = n_i / (1 - self.gamma3 ** t)\n",
        "\n",
        "            # Update parameters\n",
        "            new_ind = ind - self.alpha * m_hat / (torch.sqrt(n_hat) + self.tau)\n",
        "            new_population.append(new_ind)\n",
        "            fitnesses.append(compute_mse(model, inputs, targets))\n",
        "\n",
        "            # Zero gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            self.m[i] = m_i\n",
        "            self.n[i] = n_i\n",
        "\n",
        "        self.population = new_population\n",
        "        return fitnesses\n",
        "\n",
        "# Modified CoBiDE (M-CoBiDE)\n",
        "class MCoBiDE:\n",
        "    def __init__(self, population, pb=0.5, ps=0.4):\n",
        "        self.population = population\n",
        "        self.pb = pb\n",
        "        self.ps = ps\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.F = [self._sample_F() for _ in population]\n",
        "        self.CR = [self._sample_CR() for _ in population]\n",
        "\n",
        "    def _sample_F(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            return cauchy.rvs(loc=0.65, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            return cauchy.rvs(loc=1.0, scale=0.1, random_state=self.rng)\n",
        "\n",
        "    def _sample_CR(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            cr = cauchy.rvs(loc=0.1, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            cr = cauchy.rvs(loc=0.95, scale=0.1, random_state=self.rng)\n",
        "        return np.clip(cr, 0, 1)\n",
        "\n",
        "    def step(self, model, inputs, targets):\n",
        "        fitnesses = [compute_mse(model, inputs, targets) for ind in self.population]\n",
        "        best_idx = np.argmin(fitnesses)\n",
        "        new_population = []\n",
        "\n",
        "        # Compute covariance matrix for top ps proportion\n",
        "        top_indices = np.argsort(fitnesses)[:int(self.ps * len(self.population))]\n",
        "        top_pop = torch.stack([self.population[i] for i in top_indices])\n",
        "        cov = torch.cov(top_pop.T)\n",
        "        cov += 1e-6 * torch.eye(cov.shape[0])  # Add perturbation for stability\n",
        "        eigvals, eigvecs = torch.linalg.eigh(cov)\n",
        "        P = eigvecs\n",
        "\n",
        "        for i, (ind, F_i, CR_i) in enumerate(zip(self.population, self.F, self.CR)):\n",
        "            r1, r2 = self.rng.choice([j for j in range(len(self.population)) if j != i], 2, replace=False)\n",
        "            v_i = ind + F_i * (self.population[best_idx] - ind) + F_i * (self.population[r1] - self.population[r2])\n",
        "\n",
        "            r3 = self.rng.random()\n",
        "            if r3 >= self.pb:\n",
        "                u_i = ind.clone()\n",
        "                j_rand = self.rng.integers(0, len(ind))\n",
        "                for j in range(len(ind)):\n",
        "                    if self.rng.random() <= CR_i or j == j_rand:\n",
        "                        u_i[j] = v_i[j]\n",
        "            else:\n",
        "                x_prime = P.T @ ind\n",
        "                v_prime = P.T @ v_i\n",
        "                u_prime = x_prime.clone()\n",
        "                j_rand = self.rng.integers(0, len(ind))\n",
        "                for j in range(len(ind)):\n",
        "                    if self.rng.random() <= CR_i or j == j_rand:\n",
        "                        u_prime[j] = v_prime[j]\n",
        "                u_i = P @ u_prime\n",
        "\n",
        "            model.set_weights(u_i)\n",
        "            u_fitness = compute_mse(model, inputs, targets)\n",
        "            if u_fitness < fitnesses[i]:\n",
        "                new_population.append(u_i)\n",
        "                self.F[i] = self._sample_F()\n",
        "                self.CR[i] = self._sample_CR()\n",
        "            else:\n",
        "                new_population.append(ind)\n",
        "\n",
        "        self.population = new_population\n",
        "        return fitnesses\n",
        "\n",
        "# EDEAdam Algorithm\n",
        "class EDEAdam:\n",
        "    def __init__(self, model, pop_size=50, max_evals=25000, exchange_interval=5):\n",
        "        self.model = model\n",
        "        self.pop_size = pop_size\n",
        "        self.max_evals = max_evals\n",
        "        self.exchange_interval = exchange_interval\n",
        "        self.dim = sum(p.numel() for p in model.parameters())\n",
        "        # Verify dimension\n",
        "        expected_dim = model.input_size * model.hidden_size + model.hidden_size + model.hidden_size * model.output_size + model.output_size\n",
        "        assert self.dim == expected_dim, f\"Dimension mismatch: got {self.dim}, expected {expected_dim}\"\n",
        "        # Initialize population\n",
        "        self.population = [torch.rand(self.dim) * 2 - 1 for _ in range(pop_size)]\n",
        "        self.sub_pop1 = self.population[:pop_size//2]\n",
        "        self.sub_pop2 = self.population[pop_size//2:]\n",
        "        self.p_adam = PAdam(self.sub_pop1)\n",
        "        self.m_cobide = MCoBiDE(self.sub_pop2)\n",
        "\n",
        "    def run(self, inputs, targets):\n",
        "        t = 1\n",
        "        eval_count = 0\n",
        "        best_fitness = float('inf')\n",
        "        best_individual = None\n",
        "\n",
        "        while eval_count < self.max_evals:\n",
        "            fitness1 = self.p_adam.step(self.model, inputs, targets, t)\n",
        "            fitness2 = self.m_cobide.step(self.model, inputs, targets)\n",
        "            eval_count += len(self.sub_pop1) + len(self.sub_pop2)\n",
        "\n",
        "            best_idx1, worst_idx1 = np.argmin(fitness1), np.argmax(fitness1)\n",
        "            best_idx2, worst_idx2 = np.argmin(fitness2), np.argmax(fitness2)\n",
        "\n",
        "            if min(fitness1 + fitness2) < best_fitness:\n",
        "                best_fitness = min(fitness1 + fitness2)\n",
        "                best_individual = self.sub_pop1[best_idx1] if fitness1[best_idx1] < fitness2[best_idx2] else self.sub_pop2[best_idx2]\n",
        "\n",
        "            if t % self.exchange_interval == 0:\n",
        "                if fitness1[best_idx1] < fitness2[worst_idx2]:\n",
        "                    self.sub_pop2[worst_idx2] = self.sub_pop1[best_idx1].clone()\n",
        "                if fitness2[best_idx2] < fitness1[worst_idx1]:\n",
        "                    self.sub_pop1[worst_idx1] = self.sub_pop2[best_idx2].clone()\n",
        "\n",
        "            t += 1\n",
        "\n",
        "        self.model.set_weights(best_individual)\n",
        "        return best_fitness, eval_count\n",
        "\n",
        "# Load and preprocess Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target  # Use class indices for accuracy computation\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode targets for MSE optimization\n",
        "y_train_onehot = np.zeros((y_train.shape[0], 3))\n",
        "y_train_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
        "y_val_onehot = np.zeros((y_val.shape[0], 3))\n",
        "y_val_onehot[np.arange(y_val.shape[0]), y_val] = 1\n",
        "y_test_onehot = np.zeros((y_test.shape[0], 3))\n",
        "y_test_onehot[np.arange(y_test.shape[0]), y_test] = 1\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_onehot = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_onehot = torch.tensor(y_val_onehot, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_onehot = torch.tensor(y_test_onehot, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "pop_size_values = [50, 100, 200]\n",
        "max_evals_values = [1000, 2500, 5000]\n",
        "exchange_interval_values = [5, 10, 20]\n",
        "\n",
        "# Grid search\n",
        "best_val_accuracy = 0\n",
        "best_params = {}\n",
        "best_model_state = None\n",
        "best_fitness = float('inf')\n",
        "best_eval_count = 0\n",
        "\n",
        "for pop_size in pop_size_values:\n",
        "    for max_evals in max_evals_values:\n",
        "        for exchange_interval in exchange_interval_values:\n",
        "            print(f\"Testing: pop_size={pop_size}, max_evals={max_evals}, exchange_interval={exchange_interval}\")\n",
        "            model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "            ede_adam = EDEAdam(model, pop_size=pop_size, max_evals=max_evals, exchange_interval=exchange_interval)\n",
        "            fitness, eval_count = ede_adam.run(X_train, y_train_onehot)\n",
        "            val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "            print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_params = {\n",
        "                    'pop_size': pop_size,\n",
        "                    'max_evals': max_evals,\n",
        "                    'exchange_interval': exchange_interval\n",
        "                }\n",
        "                best_model_state = model.state_dict()\n",
        "                best_fitness = fitness\n",
        "                best_eval_count = eval_count\n",
        "\n",
        "# Load best model state\n",
        "model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate on train, validation, and test sets\n",
        "model.eval()\n",
        "train_accuracy = compute_accuracy(model, X_train, y_train)\n",
        "val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "test_accuracy = compute_accuracy(model, X_test, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"pop_size: {best_params['pop_size']}\")\n",
        "print(f\"max_evals: {best_params['max_evals']}\")\n",
        "print(f\"exchange_interval: {best_params['exchange_interval']}\")\n",
        "print(f\"Best training MSE: {best_fitness:.6f}\")\n",
        "print(f\"Training accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.6f}\")\n",
        "print(f\"Total evaluations: {best_eval_count}\")"
      ],
      "metadata": {
        "id": "jn1KWCvMuPjG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "81e966cb-14f9-43c4-93ee-85dff7613795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: pop_size=50, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.555556\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.555556\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=20\n",
            "Validation accuracy: 0.259259\n",
            "Testing: pop_size=50, max_evals=2500, exchange_interval=5\n",
            "Validation accuracy: 0.296296\n",
            "Testing: pop_size=50, max_evals=2500, exchange_interval=10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8676fa763051>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mede_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEDEAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexchange_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexchange_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mfitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mede_adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation accuracy: {val_accuracy:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8676fa763051>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0meval_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mfitness1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mfitness2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_cobide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0meval_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_pop1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_pop2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8676fa763051>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, model, inputs, targets, t)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8676fa763051>\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mw2_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw2_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw2_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiQUaNZwuP3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}