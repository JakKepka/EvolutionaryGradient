{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbq0NC-Mp0SJ"
      },
      "source": [
        "# Wine DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V35ZbSkcpz9p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Przygotowanie danych: wczytanie, skalowanie, podział i DataLoadery ---\n",
        "\n",
        "# Wczytanie danych\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standaryzacja cech\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Podział na zbiór train+val (60%) i test (40%) z zachowaniem rozkładu klas (stratyfikacja)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.40, random_state=42\n",
        ")\n",
        "\n",
        "# Podział train+val na train (60%) i val (20%) także stratyfikacja\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, stratify=y_train_val, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Konwersja do tensorów PyTorch\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Tworzenie datasetów i DataLoaderów\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMjGm9tFT4p2"
      },
      "source": [
        "# MLP Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kO4WMisT38z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.neural_network import MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UygnAJUTvfra"
      },
      "source": [
        "# DEAW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "xjrbqhCvvMTI",
        "outputId": "8345f796-bf98-4cb4-d497-50c805516169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing NP=30, F=0.3, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.3, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.3, CR=0.7, max_generations=150\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=50\n",
            "Validation accuracy: 0.905660\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.3, CR=0.9, max_generations=150\n",
            "Validation accuracy: 0.962264\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=50\n",
            "Validation accuracy: 0.716981\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing NP=30, F=0.3, CR=1.0, max_generations=150\n",
            "Validation accuracy: 0.641509\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=50\n",
            "Validation accuracy: 0.886792\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=100\n",
            "Validation accuracy: 0.924528\n",
            "Testing NP=30, F=0.5, CR=0.7, max_generations=150\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-245588a1a850>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# Perform grid search using validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_deaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Evaluate final model on train, validation, and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-245588a1a850>\u001b[0m in \u001b[0;36mgrid_search_deaw\u001b[0;34m(train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing NP={NP}, F={F}, CR={CR}, max_generations={max_generations}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_deaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_generations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_generations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation accuracy: {val_accuracy:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-245588a1a850>\u001b[0m in \u001b[0;36mtrain_deaw\u001b[0;34m(model, train_loader, device, NP, F, CR, max_generations, initial_lower, initial_upper)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mu_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_u\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-245588a1a850>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(weights, model, train_loader, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "\n",
        "# Compute accuracy for evaluation\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Fitness function using cross-entropy loss\n",
        "def fitness(weights, model, train_loader, device):\n",
        "    model.set_weights(weights.to(device))\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "# DEAW Algorithm\n",
        "def train_deaw(model, train_loader, device, NP=50, F=0.5, CR=0.9, max_generations=100, initial_lower=-1.0, initial_upper=1.0):\n",
        "    num_weights = sum(p.numel() for p in model.parameters())\n",
        "    lower_bounds = np.full(num_weights, initial_lower)\n",
        "    upper_bounds = np.full(num_weights, initial_upper)\n",
        "    population = np.random.uniform(initial_lower, initial_upper, (NP, num_weights))\n",
        "    fitnesses = np.array([fitness(torch.tensor(p, dtype=torch.float32), model, train_loader, device) for p in population])\n",
        "\n",
        "    for generation in range(max_generations):\n",
        "        for i in range(NP):\n",
        "            candidates = [j for j in range(NP) if j != i]\n",
        "            a, b, c = np.random.choice(candidates, 3, replace=False)\n",
        "            v = population[a] + F * (population[b] - population[c])\n",
        "\n",
        "            for j in range(num_weights):\n",
        "                if v[j] < lower_bounds[j]:\n",
        "                    lower_bounds[j] *= 3\n",
        "                    v[j] = lower_bounds[j]\n",
        "                elif v[j] > upper_bounds[j]:\n",
        "                    upper_bounds[j] *= 3\n",
        "                    v[j] = upper_bounds[j]\n",
        "\n",
        "            u = np.copy(population[i])\n",
        "            j_rand = np.random.randint(0, num_weights)\n",
        "            for j in range(num_weights):\n",
        "                if np.random.rand() < CR or j == j_rand:\n",
        "                    u[j] = v[j]\n",
        "\n",
        "            u_tensor = torch.tensor(u, dtype=torch.float32)\n",
        "            loss_u = fitness(u_tensor, model, train_loader, device)\n",
        "\n",
        "            if loss_u < fitnesses[i]:\n",
        "                population[i] = u.copy()\n",
        "                fitnesses[i] = loss_u\n",
        "\n",
        "    best_idx = np.argmin(fitnesses)\n",
        "    best_weights = torch.tensor(population[best_idx], dtype=torch.float32)\n",
        "    model.set_weights(best_weights.to(device))\n",
        "    return model\n",
        "\n",
        "# Grid search for hyperparameter optimization using validation set\n",
        "def grid_search_deaw(train_loader, val_loader, device):\n",
        "    # Define hyperparameter grid\n",
        "    param_grid = {\n",
        "        'NP': [30, 50, 70],\n",
        "        'F': [0.3, 0.5, 0.8],\n",
        "        'CR': [0.7, 0.9, 1.0],\n",
        "        'max_generations': [50, 100, 150]\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Iterate over all combinations of hyperparameters\n",
        "    for NP, F, CR, max_generations in product(param_grid['NP'], param_grid['F'], param_grid['CR'], param_grid['max_generations']):\n",
        "        print(f\"Testing NP={NP}, F={F}, CR={CR}, max_generations={max_generations}\")\n",
        "        model = MLP(input_size=13, hidden_size=16, output_size=3).to(device)\n",
        "        model = train_deaw(model, train_loader, device, NP=NP, F=F, CR=CR, max_generations=max_generations)\n",
        "        val_accuracy = compute_accuracy(model, val_loader, device)\n",
        "        print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_params = {'NP': NP, 'F': F, 'CR': CR, 'max_generations': max_generations}\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"\\nBest hyperparameters: {best_params}\")\n",
        "    print(f\"Best validation accuracy: {best_accuracy:.6f}\")\n",
        "    return best_model, best_params, best_accuracy\n",
        "\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Perform grid search using validation set\n",
        "best_model, best_params, best_val_accuracy = grid_search_deaw(train_loader, val_loader, device)\n",
        "\n",
        "# Evaluate final model on train, validation, and test sets\n",
        "train_accuracy = compute_accuracy(best_model, train_loader, device)\n",
        "val_accuracy = compute_accuracy(best_model, val_loader, device)\n",
        "test_accuracy = compute_accuracy(best_model, test_loader, device)\n",
        "print(f\"Final training accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Final validation accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Final test accuracy: {test_accuracy:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "TzLjXlh2x0AY",
        "outputId": "0692a6fc-f387-439a-c212-379ada5a90a8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d75d2e49398d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dokładność na zbiorze testowym: {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "accuracy = evaluate_model(model, test_loader, device)\n",
        "print(f\"Dokładność na zbiorze testowym: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU8cEM4ER2zY"
      },
      "source": [
        "# DE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaPDfg5Ux2Gx"
      },
      "outputs": [],
      "source": [
        "def train_de(model, train_loader, device, NP=50, F=0.5, CR=0.9, max_generations=100, initial_lower=-1.0, initial_upper=1.0):\n",
        "    num_weights = sum(p.numel() for p in model.parameters())\n",
        "    population = np.random.uniform(initial_lower, initial_upper, (NP, num_weights))\n",
        "    fitnesses = np.array([fitness(torch.tensor(p, dtype=torch.float32), model, train_loader, device) for p in population])\n",
        "    \n",
        "    for generation in range(max_generations):\n",
        "        for i in range(NP):\n",
        "            candidates = [j for j in range(NP) if j != i]\n",
        "            a, b, c = np.random.choice(candidates, 3, replace=False)\n",
        "            v = population[a] + F * (population[b] - population[c])\n",
        "\n",
        "            # Stałe ograniczenie: ograniczamy wartości do zakresu\n",
        "            v = np.clip(v, initial_lower, initial_upper)\n",
        "\n",
        "            u = np.copy(population[i])\n",
        "            j_rand = np.random.randint(0, num_weights)\n",
        "            for j in range(num_weights):\n",
        "                if np.random.rand() < CR or j == j_rand:\n",
        "                    u[j] = v[j]\n",
        "\n",
        "            u_tensor = torch.tensor(u, dtype=torch.float32)\n",
        "            loss_u = fitness(u_tensor, model, train_loader, device)\n",
        "\n",
        "            if loss_u < fitnesses[i]:\n",
        "                population[i] = u.copy()\n",
        "                fitnesses[i] = loss_u\n",
        "\n",
        "    best_idx = np.argmin(fitnesses)\n",
        "    best_weights = torch.tensor(population[best_idx], dtype=torch.float32)\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        numel = param.numel()\n",
        "        param.data = best_weights[idx:idx+numel].view(param.size()).to(device)\n",
        "        idx += numel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nwEczTtWn",
        "outputId": "a0808339-ed20-4bba-8954-f7e4eb67a375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing DE config: F=0.3, CR=0.7, NP=20\n",
            "Val Accuracy: 0.9811\n",
            "\n",
            "Testing DE config: F=0.3, CR=0.7, NP=50\n",
            "Val Accuracy: 0.9623\n",
            "\n",
            "Testing DE config: F=0.3, CR=0.9, NP=20\n",
            "Val Accuracy: 0.9623\n",
            "\n",
            "Testing DE config: F=0.3, CR=0.9, NP=50\n",
            "Val Accuracy: 0.9623\n",
            "\n",
            "Testing DE config: F=0.5, CR=0.7, NP=20\n",
            "Val Accuracy: 0.8868\n",
            "\n",
            "Testing DE config: F=0.5, CR=0.7, NP=50\n",
            "Val Accuracy: 0.9245\n",
            "\n",
            "Testing DE config: F=0.5, CR=0.9, NP=20\n",
            "Val Accuracy: 0.9623\n",
            "\n",
            "Testing DE config: F=0.5, CR=0.9, NP=50\n",
            "Val Accuracy: 0.9811\n",
            "\n",
            "Testing DE config: F=0.7, CR=0.7, NP=20\n",
            "Val Accuracy: 0.9245\n",
            "\n",
            "Testing DE config: F=0.7, CR=0.7, NP=50\n",
            "Val Accuracy: 0.9057\n",
            "\n",
            "Testing DE config: F=0.7, CR=0.9, NP=20\n",
            "Val Accuracy: 0.9623\n",
            "\n",
            "Testing DE config: F=0.7, CR=0.9, NP=50\n",
            "Val Accuracy: 0.9811\n",
            "\n",
            "[DE] Best hyperparams: F=0.3, CR=0.7, NP=20\n",
            "[DE] Best Val Accuracy: 0.98%\n",
            "[DE] Test Accuracy: 95.83%\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# Hiperparametry DE do przeszukania\n",
        "param_grid = {\n",
        "    'F': [0.3, 0.5, 0.7],\n",
        "    'CR': [0.7, 0.9],\n",
        "    'NP': [20, 50]\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_de_params = None\n",
        "best_model = None\n",
        "\n",
        "for F, CR, NP in product(param_grid['F'], param_grid['CR'], param_grid['NP']):\n",
        "    print(f\"\\nTesting DE config: F={F}, CR={CR}, NP={NP}\")\n",
        "\n",
        "    model_de = MLP().to(device)\n",
        "    train_de(model_de, train_loader, device, NP=NP, F=F, CR=CR, max_generations=100)\n",
        "\n",
        "    # Ewaluacja na zbiorze walidacyjnym\n",
        "    model_de.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_de(X_val.to(device))\n",
        "        val_preds = torch.argmax(val_outputs, dim=1).cpu().numpy()\n",
        "        val_acc = accuracy_score(y_val.numpy(), val_preds)\n",
        "\n",
        "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model_de\n",
        "        best_de_params = (F, CR, NP)\n",
        "\n",
        "# Ewaluacja najlepszego modelu DE na teście\n",
        "test_acc = evaluate_model(best_model, test_loader, device)\n",
        "print(f\"\\n[DE] Best hyperparams: F={best_de_params[0]}, CR={best_de_params[1]}, NP={best_de_params[2]}\")\n",
        "print(f\"[DE] Best Val Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"[DE] Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQYC5hFBNXR2"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNITxUn2NWun",
        "outputId": "007754f7-6578-44b5-a363-eb0e3fd59424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing config: lr=0.001, batch_size=8\n",
            "Train Accuracy: 0.8302 | Val Accuracy: 0.8868\n",
            "\n",
            "Testing config: lr=0.001, batch_size=16\n",
            "Train Accuracy: 0.8113 | Val Accuracy: 0.6981\n",
            "\n",
            "Testing config: lr=0.001, batch_size=32\n",
            "Train Accuracy: 0.4151 | Val Accuracy: 0.3208\n",
            "\n",
            "Testing config: lr=0.001, batch_size=64\n",
            "Train Accuracy: 0.4906 | Val Accuracy: 0.5283\n",
            "\n",
            "Testing config: lr=0.01, batch_size=8\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9057\n",
            "\n",
            "Testing config: lr=0.01, batch_size=16\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9623\n",
            "\n",
            "Testing config: lr=0.01, batch_size=32\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9434\n",
            "\n",
            "Testing config: lr=0.01, batch_size=64\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9434\n",
            "\n",
            "Testing config: lr=0.1, batch_size=8\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9623\n",
            "\n",
            "Testing config: lr=0.1, batch_size=16\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9811\n",
            "\n",
            "Testing config: lr=0.1, batch_size=32\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9623\n",
            "\n",
            "Testing config: lr=0.1, batch_size=64\n",
            "Train Accuracy: 1.0000 | Val Accuracy: 0.9623\n",
            "\n",
            "Best Params: lr=0.1, batch_size=16\n",
            "Best Val Accuracy: 0.9811\n",
            "Test Accuracy: 0.9444\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Grid: tylko lr i batch_size\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [8, 16, 32, 64]\n",
        "}\n",
        "\n",
        "n_epochs = 30\n",
        "best_val_acc = 0.0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "for lr, batch_size in product(param_grid['lr'], param_grid['batch_size']):\n",
        "    print(f\"\\nTesting config: lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    model = MLP().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Nowy DataLoader\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Trenowanie modelu\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Validation accuracy\n",
        "        val_outputs = model(X_val.to(device))\n",
        "        val_preds = torch.argmax(val_outputs, dim=1).cpu().numpy()\n",
        "        val_acc = accuracy_score(y_val.numpy(), val_preds)\n",
        "\n",
        "        # Training accuracy\n",
        "        train_outputs = model(X_train.to(device))\n",
        "        train_preds = torch.argmax(train_outputs, dim=1).cpu().numpy()\n",
        "        train_acc = accuracy_score(y_train.numpy(), train_preds)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_acc:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_params = (lr, batch_size)\n",
        "        best_model = model\n",
        "\n",
        "print(f\"\\nBest Params: lr={best_params[0]}, batch_size={best_params[1]}\")\n",
        "print(f\"Best Val Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "evaluate(best_model, X_test, y_test, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se7E7wsRuD57"
      },
      "source": [
        "# EDEADAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_s6i_g89_tU",
        "outputId": "358402f7-7c89-4cb0-ffe0-180f2603e4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: pop_size=25, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 1.000000\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=25, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.924528\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=25, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.924528\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=50, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=100, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=100, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=5\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=10\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=25\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=50\n",
            "Validation accuracy: 0.867925\n",
            "Testing: pop_size=250, max_evals=500, exchange_interval=100\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=5\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=10\n",
            "Validation accuracy: 0.962264\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=25\n",
            "Validation accuracy: 0.943396\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=50\n",
            "Validation accuracy: 0.981132\n",
            "Testing: pop_size=250, max_evals=1000, exchange_interval=100\n",
            "Validation accuracy: 0.962264\n",
            "\n",
            "Best Hyperparameters:\n",
            "pop_size: 25\n",
            "max_evals: 500\n",
            "exchange_interval: 25\n",
            "Best training cross-entropy loss: 0.000013\n",
            "Training accuracy: 1.000000\n",
            "Validation accuracy: 1.000000\n",
            "Test accuracy: 0.930556\n",
            "Total evaluations: 500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.stats import cauchy\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Compute cross-entropy loss for a batch\n",
        "def compute_loss(model, inputs, targets):\n",
        "    outputs = model(inputs)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    return loss_fn(outputs, targets).item()\n",
        "\n",
        "# Compute accuracy for a batch\n",
        "def compute_accuracy(model, inputs, targets):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct = (predicted == targets).float().sum()\n",
        "        accuracy = correct / inputs.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "# Population-based Adam (P-Adam) with batch processing\n",
        "class PAdam:\n",
        "    def __init__(self, population, alpha=0.1, gamma1=0.9, gamma2=0.99, gamma3=0.999, tau=1e-7):\n",
        "        self.population = population\n",
        "        self.alpha = alpha\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.gamma3 = gamma3\n",
        "        self.tau = tau\n",
        "        self.m = [torch.zeros_like(ind) for ind in population]  # First moment\n",
        "        self.n = [torch.zeros_like(ind) for ind in population]  # Second moment\n",
        "\n",
        "    def step(self, model, data_loader, t):\n",
        "        fitnesses = []\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        for inputs, targets in data_loader:\n",
        "            batch_fitnesses = []\n",
        "            for i, (ind, m_i, n_i) in enumerate(zip(self.population, self.m, self.n)):\n",
        "                model.set_weights(ind)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, targets)\n",
        "                loss.backward()\n",
        "\n",
        "                # Compute gradients\n",
        "                grads = torch.cat([\n",
        "                    model.fc1.weight.grad.flatten(),\n",
        "                    model.fc1.bias.grad.flatten(),\n",
        "                    model.fc2.weight.grad.flatten(),\n",
        "                    model.fc2.bias.grad.flatten()\n",
        "                ])\n",
        "\n",
        "                # Update moments\n",
        "                m_i = self.gamma1 * m_i + (1 - self.gamma1) * grads\n",
        "                n_i = self.gamma2 * n_i + (1 - self.gamma3) * (grads ** 2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_hat = m_i / (1 - self.gamma1 ** t)\n",
        "                n_hat = n_i / (1 - self.gamma3 ** t)\n",
        "\n",
        "                # Update parameters\n",
        "                new_ind = ind - self.alpha * m_hat / (torch.sqrt(n_hat) + self.tau)\n",
        "                self.population[i] = new_ind\n",
        "                batch_fitnesses.append(compute_loss(model, inputs, targets))\n",
        "\n",
        "                # Zero gradients\n",
        "                model.zero_grad()\n",
        "\n",
        "                self.m[i] = m_i\n",
        "                self.n[i] = n_i\n",
        "\n",
        "            fitnesses.append(batch_fitnesses)\n",
        "\n",
        "        # Average fitness across batches\n",
        "        fitnesses = np.mean(fitnesses, axis=0).tolist()\n",
        "        return fitnesses\n",
        "\n",
        "# Modified CoBiDE (M-CoBiDE) with batch processing\n",
        "class MCoBiDE:\n",
        "    def __init__(self, population, pb=0.5, ps=0.4):\n",
        "        self.population = population\n",
        "        self.pb = pb\n",
        "        self.ps = ps\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.F = [self._sample_F() for _ in population]\n",
        "        self.CR = [self._sample_CR() for _ in population]\n",
        "\n",
        "    def _sample_F(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            return cauchy.rvs(loc=0.65, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            return cauchy.rvs(loc=1.0, scale=0.1, random_state=self.rng)\n",
        "\n",
        "    def _sample_CR(self):\n",
        "        r = self.rng.random()\n",
        "        if r < 0.5:\n",
        "            cr = cauchy.rvs(loc=0.1, scale=0.1, random_state=self.rng)\n",
        "        else:\n",
        "            cr = cauchy.rvs(loc=0.95, scale=0.1, random_state=self.rng)\n",
        "        return np.clip(cr, 0, 1)\n",
        "\n",
        "    def step(self, model, data_loader):\n",
        "        fitnesses = []\n",
        "        for inputs, targets in data_loader:\n",
        "            batch_fitnesses = [compute_loss(model, inputs, targets) for ind in self.population]\n",
        "            best_idx = np.argmin(batch_fitnesses)\n",
        "            new_population = []\n",
        "\n",
        "            # Compute covariance matrix for top ps proportion\n",
        "            top_indices = np.argsort(batch_fitnesses)[:int(self.ps * len(self.population))]\n",
        "            top_pop = torch.stack([self.population[i] for i in top_indices])\n",
        "            cov = torch.cov(top_pop.T)\n",
        "            cov += 1e-6 * torch.eye(cov.shape[0])  # Add perturbation for stability\n",
        "            eigvals, eigvecs = torch.linalg.eigh(cov)\n",
        "            P = eigvecs\n",
        "\n",
        "            for i, (ind, F_i, CR_i) in enumerate(zip(self.population, self.F, self.CR)):\n",
        "                r1, r2 = self.rng.choice([j for j in range(len(self.population)) if j != i], 2, replace=False)\n",
        "                v_i = ind + F_i * (self.population[best_idx] - ind) + F_i * (self.population[r1] - self.population[r2])\n",
        "\n",
        "                r3 = self.rng.random()\n",
        "                if r3 >= self.pb:\n",
        "                    u_i = ind.clone()\n",
        "                    j_rand = self.rng.integers(0, len(ind))\n",
        "                    for j in range(len(ind)):\n",
        "                        if self.rng.random() <= CR_i or j == j_rand:\n",
        "                            u_i[j] = v_i[j]\n",
        "                else:\n",
        "                    x_prime = P.T @ ind\n",
        "                    v_prime = P.T @ v_i\n",
        "                    u_prime = x_prime.clone()\n",
        "                    j_rand = self.rng.integers(0, len(ind))\n",
        "                    for j in range(len(ind)):\n",
        "                        if self.rng.random() <= CR_i or j == j_rand:\n",
        "                            u_prime[j] = v_prime[j]\n",
        "                    u_i = P @ u_prime\n",
        "\n",
        "                model.set_weights(u_i)\n",
        "                u_fitness = compute_loss(model, inputs, targets)\n",
        "                if u_fitness < batch_fitnesses[i]:\n",
        "                    new_population.append(u_i)\n",
        "                    self.F[i] = self._sample_F()\n",
        "                    self.CR[i] = self._sample_CR()\n",
        "                else:\n",
        "                    new_population.append(ind)\n",
        "\n",
        "            self.population = new_population\n",
        "            fitnesses.append(batch_fitnesses)\n",
        "\n",
        "        # Average fitness across batches\n",
        "        fitnesses = np.mean(fitnesses, axis=0).tolist()\n",
        "        return fitnesses\n",
        "\n",
        "# EDEAdam Algorithm with batch processing\n",
        "class EDEAdam:\n",
        "    def __init__(self, model, pop_size=50, max_evals=25000, exchange_interval=5, batch_size=32):\n",
        "        self.model = model\n",
        "        self.pop_size = pop_size\n",
        "        self.max_evals = max_evals\n",
        "        self.exchange_interval = exchange_interval\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = sum(p.numel() for p in model.parameters())\n",
        "        # Verify dimension\n",
        "        expected_dim = model.input_size * model.hidden_size + model.hidden_size + model.hidden_size * model.output_size + model.output_size\n",
        "        assert self.dim == expected_dim, f\"Dimension mismatch: got {self.dim}, expected {expected_dim}\"\n",
        "        # Initialize population\n",
        "        self.population = [torch.rand(self.dim) * 2 - 1 for _ in range(pop_size)]\n",
        "        self.sub_pop1 = self.population[:pop_size//2]\n",
        "        self.sub_pop2 = self.population[pop_size//2:]\n",
        "        self.p_adam = PAdam(self.sub_pop1)\n",
        "        self.m_cobide = MCoBiDE(self.sub_pop2)\n",
        "\n",
        "    def run(self, train_loader):\n",
        "        t = 1\n",
        "        eval_count = 0\n",
        "        best_fitness = float('inf')\n",
        "        best_individual = None\n",
        "\n",
        "        while eval_count < self.max_evals:\n",
        "            fitness1 = self.p_adam.step(self.model, train_loader, t)\n",
        "            fitness2 = self.m_cobide.step(self.model, train_loader)\n",
        "            eval_count += len(self.sub_pop1) + len(self.sub_pop2)\n",
        "\n",
        "            best_idx1, worst_idx1 = np.argmin(fitness1), np.argmax(fitness1)\n",
        "            best_idx2, worst_idx2 = np.argmin(fitness2), np.argmax(fitness2)\n",
        "\n",
        "            if min(fitness1 + fitness2) < best_fitness:\n",
        "                best_fitness = min(fitness1 + fitness2)\n",
        "                best_individual = self.sub_pop1[best_idx1] if fitness1[best_idx1] < fitness2[best_idx2] else self.sub_pop2[best_idx2]\n",
        "\n",
        "            if t % self.exchange_interval == 0:\n",
        "                if fitness1[best_idx1] < fitness2[worst_idx2]:\n",
        "                    self.sub_pop2[worst_idx2] = self.sub_pop1[best_idx1].clone()\n",
        "                if fitness2[best_idx2] < fitness1[worst_idx1]:\n",
        "                    self.sub_pop1[worst_idx1] = self.sub_pop2[best_idx2].clone()\n",
        "\n",
        "            t += 1\n",
        "\n",
        "        self.model.set_weights(best_individual)\n",
        "        return best_fitness, eval_count\n",
        "\n",
        "# Create DataLoader for training\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "pop_size_values = [25, 50, 100, 250]\n",
        "max_evals_values = [500, 1000]\n",
        "exchange_interval_values = [5, 10, 25, 50, 100]\n",
        "\n",
        "# Grid search\n",
        "best_val_accuracy = 0\n",
        "best_params = {}\n",
        "best_model_state = None\n",
        "best_fitness = float('inf')\n",
        "best_eval_count = 0\n",
        "\n",
        "for pop_size in pop_size_values:\n",
        "    for max_evals in max_evals_values:\n",
        "        for exchange_interval in exchange_interval_values:\n",
        "            print(f\"Testing: pop_size={pop_size}, max_evals={max_evals}, exchange_interval={exchange_interval}\")\n",
        "            model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "            ede_adam = EDEAdam(model, pop_size=pop_size, max_evals=max_evals, exchange_interval=exchange_interval, batch_size=batch_size)\n",
        "            fitness, eval_count = ede_adam.run(train_loader)\n",
        "            val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "            print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_params = {\n",
        "                    'pop_size': pop_size,\n",
        "                    'max_evals': max_evals,\n",
        "                    'exchange_interval': exchange_interval\n",
        "                }\n",
        "                best_model_state = model.state_dict()\n",
        "                best_fitness = fitness\n",
        "                best_eval_count = eval_count\n",
        "\n",
        "# Load best model state\n",
        "model = MLP(input_size=13, hidden_size=16, output_size=3)\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate on train, validation, and test sets\n",
        "model.eval()\n",
        "train_accuracy = compute_accuracy(model, X_train, y_train)\n",
        "val_accuracy = compute_accuracy(model, X_val, y_val)\n",
        "test_accuracy = compute_accuracy(model, X_test, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"pop_size: {best_params['pop_size']}\")\n",
        "print(f\"max_evals: {best_params['max_evals']}\")\n",
        "print(f\"exchange_interval: {best_params['exchange_interval']}\")\n",
        "print(f\"Best training cross-entropy loss: {best_fitness:.6f}\")\n",
        "print(f\"Training accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Validation accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.6f}\")\n",
        "print(f\"Total evaluations: {best_eval_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6DFviyOWMRS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
